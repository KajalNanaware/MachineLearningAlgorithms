{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master=\"yarn\"\n",
    "launcher.num_executors=4\n",
    "launcher.executor_cores=2\n",
    "launcher.executor_memory='6000m'\n",
    "launcher.packages=[\"com.github.master:spark-stemming_2.10:0.2.0\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data Exploration: -Loading data and Extracting 'stars' and respective review's 'text'\n",
    "\n",
    "# Finding distribution of stars over each text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://bdu-hm19:8088/proxy/application_1544057569429_0001\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = yarn, app id = application_1544057569429_0001)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stars: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|stars|                text|\n",
      "+-----+--------------------+\n",
      "|    5|Love the staff, l...|\n",
      "|    5|Super simple plac...|\n",
      "|    5|Small unassuming ...|\n",
      "|    5|Lester's is locat...|\n",
      "|    4|Love coming here....|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "review_df: org.apache.spark.sql.DataFrame = [stars: bigint, text: string]\n",
       "res0: Long = 5261669\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val review_df=spark.read.json(\"/hadoop-user/data/yelp/review.json\")\n",
    "              .select(($\"stars\").alias(\"stars\"),($\"text\").alias(\"text\"))\n",
    "\n",
    "review_df.cache()\n",
    "review_df.printSchema()\n",
    "review_df.show(5)\n",
    "review_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Feature Engineering: -Creating column 'rating' for categorizing stars into two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|stars|                text|rating|\n",
      "+-----+--------------------+------+\n",
      "|    5|Love the staff, l...|   1.0|\n",
      "|    5|Super simple plac...|   1.0|\n",
      "|    5|Small unassuming ...|   1.0|\n",
      "|    5|Lester's is locat...|   1.0|\n",
      "|    4|Love coming here....|   1.0|\n",
      "|    4|Had their chocola...|   1.0|\n",
      "|    5|Cycle Pub Las Veg...|   1.0|\n",
      "|    4|Who would have gu...|   1.0|\n",
      "|    4|Always drove past...|   1.0|\n",
      "|    3|Not bad!! Love th...|   0.0|\n",
      "|    5|Love this place!\n",
      "...|   1.0|\n",
      "|    4|This is currently...|   1.0|\n",
      "|    3|Server was a litt...|   0.0|\n",
      "|    1|I thought Tidy's ...|   0.0|\n",
      "|    3|Wanted to check o...|   0.0|\n",
      "|    5|This place is awe...|   1.0|\n",
      "|    4|a must stop when ...|   1.0|\n",
      "|    1|I too have been t...|   0.0|\n",
      "|    3|Came here with my...|   0.0|\n",
      "|    3|Came here for a b...|   0.0|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature._\n",
       "import org.apache.spark.ml.linalg._\n",
       "splits: Array[Double] = Array(-Infinity, 4.0, Infinity)\n",
       "bucketizer: org.apache.spark.ml.feature.Bucketizer = bucketizer_1d7665d54ff8\n",
       "bucketedData: org.apache.spark.sql.DataFrame = [stars: bigint, text: string ... 1 more field]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "import org.apache.spark.ml.linalg._\n",
    "val splits = Array(Double.NegativeInfinity,4, Double.PositiveInfinity)\n",
    "\n",
    "val bucketizer = new Bucketizer()\n",
    "  .setInputCol(\"stars\")\n",
    "  .setOutputCol(\"rating\")\n",
    "  .setSplits(splits)\n",
    "\n",
    "// Transform original data into its bucket index.\n",
    "val bucketedData = bucketizer.transform(review_df)\n",
    "bucketedData.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -Finding disrtibution of rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|rating|count(text)|\n",
      "+------+-----------+\n",
      "|   0.0|    1785005|\n",
      "|   1.0|    3476664|\n",
      "+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketedData.createOrReplaceTempView(\"ratingData\")\n",
    "spark.sql(\"select rating,count(text) from ratingData group by rating\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -Using sampleBy() to down sample the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|rating|count|\n",
      "+------+-----+\n",
      "|   0.0| 1788|\n",
      "|   1.0| 1771|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fractionMapKey: scala.collection.immutable.Map[Double,Double] = Map(0.0 -> 0.001, 1.0 -> 5.0E-4)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fractionMapKey = Map(0.0->0.001,1.0->(0.00050))\n",
    "bucketedData.stat.sampleBy(\"rating\",fractionMapKey,50).groupBy(\"rating\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -Finding TFIDF vector of text review.\n",
    "# -Spliting data into test and training, creating pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampled_data: org.apache.spark.sql.DataFrame = [stars: bigint, text: string ... 1 more field]\n",
       "import org.apache.spark.ml.feature._\n",
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_5cf1fd84b479\n",
       "import org.apache.spark.sql.functions.udf\n",
       "removePunc: (words: Seq[String])Seq[String]\n",
       "puncRemover: org.apache.spark.ml.feature.SQLTransformer = sql_baaf90d57ce4\n",
       "stopWordRemover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_40bdcd5ea7df\n",
       "import org.apache.spark.mllib.feature.Stemmer\n",
       "stemmer: org.apache.spark.mllib.feature.Stemmer = stemmer_6e573d5c01f2\n",
       "vectorizer: org.apache.spark.ml.feature.CountVectorizer = cntVec_018d16e5f9e3\n",
       "tfidf: org.apache.spark.ml.feature.IDF = idf_a770a1ad8552\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "pipeline: org.apache.spark.ml.Pipe..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Storing Down Sampled data\n",
    "val sampled_data = bucketedData.stat.sampleBy(\"rating\",fractionMapKey,50)\n",
    "\n",
    "\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "val tokenizer = new RegexTokenizer().setMinTokenLength(3).setToLowercase(true).setInputCol(\"text\").setOutputCol(\"text_words\")\n",
    "\n",
    "//Defining a udf to remove punctuations from a sequence of words\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "def removePunc(words:Seq[String]):Seq[String]={\n",
    " return words.map(_.replaceAll(\"\\\\p{Punct}\",\" \"))\n",
    "}\n",
    "//val removePuncUDF=udf(removePunc(_:Seq[String]))\n",
    "spark.udf.register(\"removePuncUDF\",removePunc(_:Seq[String]) )\n",
    "\n",
    "//use the removePuncUDF to remove all punctuations from boilerplate_wordss\n",
    "val puncRemover = new SQLTransformer().setStatement(\"SELECT removePuncUDF(text_words) as review_words, rating from __THIS__ \")\n",
    "\n",
    "val stopWordRemover=new StopWordsRemover().setInputCol(\"review_words\").setOutputCol(\"filtered_words\")\n",
    "\n",
    "import org.apache.spark.mllib.feature.Stemmer\n",
    "val stemmer = new Stemmer().setInputCol(\"filtered_words\").setOutputCol(\"stemmed_words\")\n",
    "\n",
    "val vectorizer = new CountVectorizer().setInputCol(\"stemmed_words\").setOutputCol(\"review_BOW\").setMinDF(100)\n",
    "val tfidf = new IDF().setInputCol(\"review_BOW\").setOutputCol(\"review_TFIDF\")\n",
    "\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "val pipeline = new Pipeline().setStages(Array(tokenizer,puncRemover,stopWordRemover, stemmer, vectorizer, tfidf))\n",
    "val Array(training,testing)=sampled_data.randomSplit(Array(0.8,0.2),111)\n",
    "val preprocessing_pipeline= pipeline.fit(training)\n",
    "                                        \n",
    "val training_cleaned=preprocessing_pipeline.transform(training)\n",
    "val testing_cleaned=preprocessing_pipeline.transform(testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Machine Learning models:-Creating Gradient boosted classifier model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|       stemmed_words|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|   0.0|       0.0|[0.66052134867120...|[came, locat, boc...|\n",
      "|   0.0|       0.0|[0.70749349361436...|[i v, times , wel...|\n",
      "|   0.0|       0.0|[0.78836886859910...|[give, zero , wou...|\n",
      "|   0.0|       0.0|[0.68090435030942...|[went, 11pm, sund...|\n",
      "|   0.0|       0.0|[0.69306604895833...|[place, crap , do...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for GBT on test data = 0.7967578497750715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "gbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_a10209091230\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tgbtc_a10209091230-maxDepth: 2\n",
       "}, {\n",
       "\tgbtc_a10209091230-maxDepth: 5\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_793803026bb5\n",
       "cv_gb: org.apache.spark.ml.tuning.CrossValidator = cv_8e4e..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "// Train a GBT model.\n",
    "val gbt = new GBTClassifier().setLabelCol(\"rating\").setFeaturesCol(\"review_TFIDF\")\n",
    "\n",
    "//Setting param grid parameters\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, Array(2, 5))\n",
    "             .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val cv_gb = new CrossValidator().setEstimator(gbt).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val model = cv_gb.fit(training_cleaned)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions_gbt = model.transform(testing_cleaned)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_words\").show(5)\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for GBT on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -Creating Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|       stemmed_words|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|   0.0|       0.0|[0.50387068266630...|[came, locat, boc...|\n",
      "|   0.0|       0.0|[0.50387068266630...|[i v, times , wel...|\n",
      "|   0.0|       0.0|[0.50387068266630...|[give, zero , wou...|\n",
      "|   0.0|       0.0|[0.50387068266630...|[went, 11pm, sund...|\n",
      "|   0.0|       0.0|[0.50387068266630...|[place, crap , do...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for LR on test data = 0.588911104042653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_3be6a861253d\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_3be6a861253d-elasticNetParam: 0.2,\n",
       "\tlogreg_3be6a861253d-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_3be6a861253d-elasticNetParam: 0.7,\n",
       "\tlogreg_3be6a861253d-regParam: 0.5\n",
       "}, {\n",
       "\tlogreg_3be6a861253d-elasticNetParam: 0.2,\n",
       "\tlogreg_3be6a861253d-regParam: 2.0\n",
       "}, {\n",
       "\tlogreg_3be6a861253d-elasticNetParam: 0.7,\n",
       "\tlogreg_3be6a861253d-regParam: 2.0\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_a78e7e32e0c8\n",
       "cv: org.apache.spark.ml.tuning.CrossValidator = cv_d38e..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "//Building logistic regression model\n",
    "val lr = new LogisticRegression().setLabelCol(\"rating\").setFeaturesCol(\"review_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, Array(0.5, 2.0))\n",
    "             .addGrid(lr.elasticNetParam, Array(0.2,0.7))\n",
    "             .build()\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "val cv = new CrossValidator().setEstimator(lr).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "//Fit the training data to the pipeline\n",
    "val model = cv.fit(training_cleaned)\n",
    "\n",
    "// Make predictions.\n",
    "val predictions_lr = model.transform(testing_cleaned)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_words\").show(5)\n",
    "\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for LR on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -Creating Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------------------+\n",
      "|rating|prediction|         probability|       stemmed_words|\n",
      "+------+----------+--------------------+--------------------+\n",
      "|   0.0|       0.0|[0.50387068266630...|[came, locat, boc...|\n",
      "|   0.0|       0.0|[0.50387068266630...|[i v, times , wel...|\n",
      "|   0.0|       0.0|[0.50387068266630...|[give, zero , wou...|\n",
      "|   0.0|       0.0|[0.50387068266630...|[went, 11pm, sund...|\n",
      "|   0.0|       0.0|[0.50387068266630...|[place, crap , do...|\n",
      "+------+----------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Area under ROC curve(AUC) for RF on test data = 0.588911104042653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning._\n",
       "import org.apache.spark.ml.evaluation._\n",
       "import org.apache.spark.ml.feature._\n",
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_884e228e416b\n",
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\trfc_884e228e416b-maxDepth: 2,\n",
       "\trfc_884e228e416b-numTrees: 5\n",
       "}, {\n",
       "\trfc_884e228e416b-maxDepth: 2,\n",
       "\trfc_884e228e416b-numTrees: 10\n",
       "}, {\n",
       "\trfc_884e228e416b-maxDepth: 5,\n",
       "\trfc_884e228e416b-numTrees: 5\n",
       "}, {\n",
       "\trfc_884e228e416b-maxDepth: 5,\n",
       "\trfc_884e228e416b-numTrees: 10\n",
       "})\n",
       "evaluator: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_24c88c948c2a\n",
       "cv_rf: org.apache.spark.ml.tuning.CrossValidator = cv_a260af710..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning._\n",
    "import org.apache.spark.ml.evaluation._\n",
    "import org.apache.spark.ml.feature._\n",
    "\n",
    "//Building Random Forest model\n",
    "val rf = new RandomForestClassifier().setLabelCol(\"rating\").setFeaturesCol(\"review_TFIDF\")\n",
    "val paramGrid =new ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, Array(2, 5))\n",
    "             .addGrid(rf.numTrees, Array(5, 10))\n",
    "             .build()\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"rating\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "val cv_rf = new CrossValidator().setEstimator(rf).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(3)\n",
    "\n",
    "val model = cv_rf.fit(training_cleaned)\n",
    "\n",
    "val predictions_rf = model.transform(testing_cleaned)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"rating\", \"prediction\", \"probability\", \"stemmed_words\").show(5)\n",
    "val AUC = evaluator.evaluate(predictions)\n",
    "println(s\"Area under ROC curve(AUC) for RF on test data = $AUC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating views for prediction dataframes of all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_gbt.createOrReplaceTempView(\"predictionsGbt_view\")\n",
    "predictions_lr.createOrReplaceTempView(\"predictionsLR_view\")\n",
    "predictions_rf.createOrReplaceTempView(\"predictionsRF_view\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling prediction from all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|prediction_ensemble|rating|\n",
      "+-------------------+------+\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "|                0.0|   0.0|\n",
      "+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ensemble: org.apache.spark.sql.DataFrame = [prediction_ensemble: double, rating: double]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Sql  query to select if A=B or A=C then A else B\n",
    "val ensemble = spark.sql(\"select CASE WHEN gbt.prediction = lr.prediction OR gbt.prediction = rf.prediction THEN  gbt.prediction ELSE rf.prediction END AS prediction_ensemble, gbt.rating FROM predictionsGbt_view gbt INNER JOIN predictionsLR_view lr ON gbt.rating =lr.rating INNER JOIN predictionsRF_view rf ON lr.rating=rf.rating\")\n",
    "\n",
    "ensemble.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Area under ROC curve for ensembled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.742132331646631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\n",
       "import org.apache.spark.mllib.evaluation._\n",
       "predictionsAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[9232] at map at <console>:102\n",
       "metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@32c3ccdc\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "import org.apache.spark.mllib.evaluation._\n",
    "\n",
    "//Converting the ensemble dataframe to an rdd of the form (prediction_ensemble, rating)\n",
    "val predictionsAndLabels = ensemble.selectExpr(\"cast(prediction_ensemble as Double) prediction_ensemble\",\"cast(rating as Double) rating\")\n",
    ".rdd.map(row=>(row.getAs[Double](\"prediction_ensemble\"),row.getAs[Double](\"rating\")))\n",
    "\n",
    "//Compute the accuracy of the ensemble predictions\n",
    "val metrics = new BinaryClassificationMetrics(predictionsAndLabels)\n",
    "\n",
    "println(metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
